{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Train+Test+Report.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "pycharm": {
      "stem_cell": {
        "cell_type": "raw",
        "source": [],
        "metadata": {
          "collapsed": false
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ITir__zO-J6H"
      },
      "source": [
        "# **Отчет**\n",
        "В ходе данной работы была обучена генератавно-состязательная сеть (*GAN*) состоящая из Генератора и Дискриминатора. Получен результат метрики **FID = 18.939**\n",
        "\n",
        "Первоначально была выбрана архитектура из оригинальной статьи DCGAN, где количество карт признаков (feature maps) на слоях генератора уменьшалось от 128 до 64 с фактором 2 (для дискриминатора - наоборот). Данная реализация показывала плохие результаты уже на этапе визульного анализа.\n",
        "\n",
        "Архитектура была переработана, количество карт признаков увеличено до 512 (512->256->128->64->3). (реализация ниже в коде) Были получены следующие результаты...\n",
        "\n",
        "Использование LeakyReLU(0.2) в генераторе:\n",
        "\n",
        "*   FID (50 эпох) - 64.925\n",
        "*   FID (100 эпох) - 55.096\n",
        "*   FID (200 эпох) - 28.702\n",
        "*   FID (300 эпох) - 27.431\n",
        "\n",
        "Использование ReLU в генераторе:\n",
        "\n",
        "*   FID (50 эпох) - 41.219\n",
        "*   FID (100 эпох) -29.052\n",
        "*   FID (200 эпох) - 23.548\n",
        "*   FID (300 эпох) - 28.231\n",
        "\n",
        "Таким образом ReLU активации показывают лучшие результаты по сравнению с LeakyReLU, а обучение после 200 эпох отрицательно сказывается на результате.\n",
        "\n",
        "Также было решено протестировать использование 1024 карт признаков (1024->512->256->128->64->3). Время обучения при этом увеличилось практически в два раза:\n",
        "\n",
        "*   FID (50 эпох) - 29.109\n",
        "*   FID (100 эпох) - 18.939\n",
        "*   FID (200 эпох) - 24.474\n",
        "*   FID (300 эпох) - 25.896\n",
        "\n",
        "Вывод: количество карт признаков играет существенную роль при обучении генеративно-состязательных сетей. Несмотря на то, что зачастую с увеличением количества параметров у модели появляется склонность к переобучению, в случае с GAN это необходимо.\n",
        " \n",
        "Также было отмечено, что примерно после 50 эпох изображения начинают выглядеть малоконтастными. Линейная коррекция контраста (функция contrasting_Tensor()) полностью решает эту проблему."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KYlpZhFJJ9J3"
      },
      "source": [
        "# **Colab stuff**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R0WGWY6j8Y-d"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xQATQ6Jv8gVi"
      },
      "source": [
        "import zipfile\n",
        "with zipfile.ZipFile('drive/MyDrive/cats/archive.zip', 'r') as zip_obj:\n",
        "   # Extract all the contents of zip file\n",
        "   zip_obj.extractall('dataset')"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-T8HkbGU7X9m"
      },
      "source": [
        "# **Imports**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HmErqQJGTVdX"
      },
      "source": [
        "import os"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AdUUFrNITV37"
      },
      "source": [
        "import numpy as np \n",
        "import pandas as pd\n",
        "from skimage.io import imsave\n",
        "from skimage.util import img_as_ubyte"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5o3DTP8HVI8g"
      },
      "source": [
        "from torch.utils.data import DataLoader\n",
        "from torchvision.datasets import ImageFolder\n",
        "from torchvision import transforms\n",
        "import torch\n",
        "from torch import device\n",
        "import torch.nn as nn\n",
        "from tqdm.notebook import tqdm\n",
        "import torch.nn.functional as F\n",
        "from torchvision.utils import save_image\n",
        "from torchvision.utils import make_grid\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ],
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JBWZRo1p7vvm"
      },
      "source": [
        "# **Initialization**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jl37sSWfZic9"
      },
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4_2ECF1TbDDA"
      },
      "source": [
        "n_epochs = 100 # number of epochs of training\n",
        "batch_size = 128 # size of the batches\n",
        "lr = 0.0002 # adam: learning rate\n",
        "b1 = 0.5 # adam: decay of first order momentum of gradient\n",
        "b2 = 0.999 # adam: decay of first order momentum of gradient\n",
        "n_cpu = 8 # number of cpu threads to use during batch generation\n",
        "latent_size = 128 # dimensionality of the latent space\n",
        "img_size = 64 # size of each image dimension\n",
        "channels = 3 # number of image channels\n",
        "\n",
        "dataset_path = \"dataset\" # path of the directory with folder containing images\n",
        "model_path = \"drive/MyDrive/gan_models/\" # path of the models to save\n",
        "sample_path = \"drive/MyDrive/images\" # path of the partial results to save"
      ],
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f68o-G_FBtDQ"
      },
      "source": [
        "# **Help functions**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Brnfj9c1ZyZj"
      },
      "source": [
        "# image normalization from [-1,1] to [0,1]\n",
        "def denorm(img_tensors):\n",
        "    \"\"\"Image normalization\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    img_tensors: Tensor\n",
        "        Tensor of images\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    img_tensors: Tensor\n",
        "        normalized images\n",
        "    \"\"\"\n",
        "    \n",
        "    return img_tensors * 0.5 + 0.5"
      ],
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zaH7XbFGveRI"
      },
      "source": [
        "# Contrast correction (linear)\n",
        "def contrasting(image):\n",
        "    \"\"\"Linear contrast correction\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        image: Tensor\n",
        "            batch of images to contrast\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        image: Tensor\n",
        "            corrected image\n",
        "    \"\"\"\n",
        "    image *= 255\n",
        "\n",
        "    R = image[:, :, :, 0]\n",
        "    G = image[:, :, :, 1]\n",
        "    B = image[:, :, :, 2]\n",
        "\n",
        "    Y = 0.2126*R + 0.7152*G + 0.0722*B\n",
        "    U = -0.0999*R - 0.3360*G + 0.4360*B\n",
        "    V = 0.6150*R - 0.5586*G - 0.0563*B\n",
        "\n",
        "    new_y = Y.view(Y.shape[0],-1)\n",
        "\n",
        "    x_min = torch.min(new_y, dim=1)[0]\n",
        "    x_max = torch.max(new_y, dim=1)[0]\n",
        "\n",
        "    out = (new_y-x_min[:,None])*255 / (x_max-x_min)[:,None]\n",
        "\n",
        "    Y = out.view(Y.shape)\n",
        "\n",
        "    R = (Y + 1.2803*V).unsqueeze(3)\n",
        "    G = (Y - 0.2148*U - 0.3805*V).unsqueeze(3)\n",
        "    B = (Y + 2.1279*U).unsqueeze(3)\n",
        "\n",
        "    output = torch.cat((R, G, B),3)\n",
        "    output = torch.clamp(output, 0, 255) / 255\n",
        "\n",
        "    return output"
      ],
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Qt49OM68Pz4"
      },
      "source": [
        "# **Data preparation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qnD0Bn4SZokt"
      },
      "source": [
        "# Set DataLoader\n",
        "train_transforms = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5]),\n",
        "])\n",
        "\n",
        "train_dataset = ImageFolder(dataset_path, train_transforms)\n",
        "\n",
        "dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=n_cpu, pin_memory=True)"
      ],
      "execution_count": 52,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cSnMcbdX8eb8"
      },
      "source": [
        "# **Models**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XDt0oIrkZrhD"
      },
      "source": [
        "# Generator\n",
        "generator = nn.Sequential(\n",
        "    # in: latent_size x 1 x 1\n",
        "\n",
        "    nn.ConvTranspose2d(latent_size, 1024, kernel_size=4, stride=1, padding=0, bias=False),\n",
        "    nn.BatchNorm2d(1024),\n",
        "    nn.ReLU(True),\n",
        "    # out: 1024 x 4 x 4\n",
        "\n",
        "    nn.ConvTranspose2d(1024, 512, kernel_size=4, stride=2, padding=1, bias=False),\n",
        "    nn.BatchNorm2d(512),\n",
        "    nn.ReLU(True),\n",
        "    # out: 512 x 8 x 8\n",
        "\n",
        "    nn.ConvTranspose2d(512, 256, kernel_size=4, stride=2, padding=1, bias=False),\n",
        "    nn.BatchNorm2d(256),\n",
        "    nn.ReLU(True),\n",
        "    # out: 256 x 16 x 16\n",
        "\n",
        "    nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1, bias=False),\n",
        "    nn.BatchNorm2d(128),\n",
        "    nn.ReLU(True),\n",
        "    # out: 128 x 32 x 32\n",
        "\n",
        "    nn.ConvTranspose2d(128, channels, kernel_size=4, stride=2, padding=1, bias=False),\n",
        "    nn.Tanh())\n",
        "    # out: 3 x 64 x 64"
      ],
      "execution_count": 53,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5VkWnWyAZv2k"
      },
      "source": [
        "# Discriminator\n",
        "discriminator = nn.Sequential(\n",
        "    # in: 3 x 64 x 64\n",
        "\n",
        "    nn.Conv2d(channels, 128, kernel_size=4, stride=2, padding=1, bias=False),\n",
        "    nn.BatchNorm2d(128),\n",
        "    nn.LeakyReLU(0.2, inplace=True),\n",
        "    # out: 128 x 32 x 32\n",
        "\n",
        "    nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1, bias=False),\n",
        "    nn.BatchNorm2d(256),\n",
        "    nn.LeakyReLU(0.2, inplace=True),\n",
        "    # out: 256 x 16 x 16\n",
        "\n",
        "    nn.Conv2d(256, 512, kernel_size=4, stride=2, padding=1, bias=False),\n",
        "    nn.BatchNorm2d(512),\n",
        "    nn.LeakyReLU(0.2, inplace=True),\n",
        "    # out: 512 x 8 x 8\n",
        "\n",
        "    nn.Conv2d(512, 1024, kernel_size=4, stride=2, padding=1, bias=False),\n",
        "    nn.BatchNorm2d(1024),\n",
        "    nn.LeakyReLU(0.2, inplace=True),\n",
        "    # out: 1024 x 4 x 4\n",
        "\n",
        "    nn.Conv2d(1024, 1, kernel_size=4, stride=1, padding=0, bias=False),\n",
        "    # out: 1 x 1 x 1\n",
        "\n",
        "    nn.Flatten(),\n",
        "    nn.Sigmoid())"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "neLOLotB6Z12"
      },
      "source": [
        "# Move models to the gpu if available else cpu\n",
        "discriminator.to(device)\n",
        "generator.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x9TpjT3282jU"
      },
      "source": [
        "# **Optimizers**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "84HAnqRg6T6I"
      },
      "source": [
        "# Create optimizers\n",
        "optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=lr, betas=(b1, b2))\n",
        "optimizer_G = torch.optim.Adam(generator.parameters(), lr=lr, betas=(b1, b2))"
      ],
      "execution_count": 56,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WJnkmBRV6iSt"
      },
      "source": [
        "# **Load models from checkpoint (optional)**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C9F3VcjMEkmo"
      },
      "source": [
        "# load checkpoint dict\n",
        "checkpoint_path = \"checkpoint/path/model_777.tar\"\n",
        "checkpoint = torch.load(checkpoint_path)\n",
        "\n",
        "# initialize models and optimizers with trained parameters\n",
        "generator.load_state_dict(checkpoint['generator_state_dict'])\n",
        "discriminator.load_state_dict(checkpoint['discriminator_state_dict'])\n",
        "optimizer_G.load_state_dict(checkpoint['optimizer_G_state_dict'])\n",
        "optimizer_D.load_state_dict(checkpoint['optimizer_D_state_dict'])\n",
        "\n",
        "# Move models to the gpu if available else cpu\n",
        "discriminator.to(device)\n",
        "generator.to(device)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bkA-47E86xsV"
      },
      "source": [
        "# **Train**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "shDgMGeJZ2rb"
      },
      "source": [
        "#To save the samples produced during epochs\n",
        "def save_samples(sample_path, index, latent_tensors, show=True):\n",
        "    \"\"\"Image grid saving\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        sample_path: string\n",
        "            path to the sampling output\n",
        "        index: int\n",
        "            index of saving\n",
        "        latent_tensors: Tensor\n",
        "            input latent tensors\n",
        "        show: bool\n",
        "            whether to show the result\n",
        "    \"\"\"\n",
        "    \n",
        "    fake_images = generator(latent_tensors).to(device)\n",
        "    fake_fname = 'generated-images-{0:0=4d}.png'.format(index)\n",
        "    save_image(denorm(fake_images), os.path.join(sample_path, fake_fname), nrow=8)\n",
        "    print('Saving', fake_fname)\n",
        "    if show:\n",
        "        fig, ax = plt.subplots(figsize=(8, 8))\n",
        "        ax.set_xticks([]); ax.set_yticks([])\n",
        "        ax.imshow(make_grid(fake_images.cpu().detach(), nrow=8).permute(1, 2, 0))"
      ],
      "execution_count": 57,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NjzTwGxHZ5DB"
      },
      "source": [
        "def train_discriminator(real_images):\n",
        "    \"\"\"Discriminator training step\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        real_images: Tensor\n",
        "            batch of real images\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        (loss, real_score, fake_score): Tuple\n",
        "            discriminator loss, discriminator score on real images,\n",
        "            discriminator score on fake images,\n",
        "    \"\"\"\n",
        "    # Clear discriminator gradients\n",
        "    optimizer_D.zero_grad()\n",
        "\n",
        "    # Pass real images through discriminator\n",
        "    real_preds = discriminator(real_images).to(device) #real images\n",
        "    real_targets = torch.ones(real_images.size(0), 1).to(device) #setting targets as 1\n",
        "    real_loss = F.binary_cross_entropy(real_preds, real_targets) #getting the loss\n",
        "    real_score = torch.mean(real_preds).item()\n",
        "    \n",
        "    # Generate fake images\n",
        "    latent = torch.randn(batch_size, latent_size, 1, 1).to(device) #generating the random noices for input image\n",
        "    fake_images = generator(latent).to(device)  #getting the fake images\n",
        "\n",
        "    # Pass fake images through discriminator\n",
        "    fake_targets = torch.zeros(fake_images.size(0), 1).to(device) #setting 0 as target for fake images\n",
        "    fake_preds = discriminator(fake_images).to(device)  #getting the predictions for fake images\n",
        "    fake_loss = F.binary_cross_entropy(fake_preds, fake_targets)  #Comparing the two scores through loss\n",
        "    fake_score = torch.mean(fake_preds).item()\n",
        "\n",
        "    # Update discriminator weights\n",
        "    loss = real_loss + fake_loss\n",
        "    loss.backward()\n",
        "    optimizer_D.step()\n",
        "    return loss.item(), real_score, fake_score"
      ],
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j738tnoNaCzk"
      },
      "source": [
        "def train_generator():\n",
        "    \"\"\"Generator training step\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        (loss, latent): Tuple\n",
        "            generator loss, batch of latent tensors\n",
        "    \"\"\"\n",
        "    # Clear generator gradients\n",
        "    optimizer_G.zero_grad()\n",
        "    \n",
        "    # Generate fake images\n",
        "    latent = torch.randn(batch_size, latent_size, 1,1).to(device) #random noice\n",
        "    fake_images = generator(latent).to(device) #fake images generated\n",
        "    \n",
        "    # Try to fool the discriminator\n",
        "    preds = discriminator(fake_images).to(device) #getting the predictions of discriminator for fake images\n",
        "    targets = torch.ones(batch_size, 1).to(device) #setting 1 as targets so the discriminator can be fooled\n",
        "    loss = F.binary_cross_entropy(preds, targets) #comparing\n",
        "    \n",
        "    # Update generator weights\n",
        "    loss.backward()\n",
        "    optimizer_G.step()\n",
        "    \n",
        "    return loss.item(),latent"
      ],
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dZjiA3oPaG0s"
      },
      "source": [
        "def fit(epochs, lr, start_idx=1, model_path=\"models\", sample_path=\"images\"):\n",
        "    \"\"\"Fit loop\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        epochs: int\n",
        "            number of epoches to train\n",
        "        lr: float\n",
        "            learning rate\n",
        "        start_idx: int\n",
        "            epoch start index\n",
        "        model_path: string\n",
        "            path of the models to save\n",
        "        sample_path: string\n",
        "            path to the sampling images output\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        (losses_g, losses_d, latent, fake_scores): Tuple\n",
        "            generator losses, discriminator losses,\n",
        "            discriminator scores on real images,\n",
        "            discriminator scores on fake images\n",
        "    \"\"\"\n",
        "    torch.cuda.empty_cache()\n",
        "    \n",
        "    os.makedirs(model_path, exist_ok=True)\n",
        "    os.makedirs(sample_path, exist_ok=True)\n",
        "\n",
        "    # Losses & scores\n",
        "    losses_g = []\n",
        "    losses_d = []\n",
        "    real_scores = []\n",
        "    fake_scores = []\n",
        "    \n",
        "    for epoch in range(epochs):\n",
        "        for real_images, _ in tqdm(dataloader):\n",
        "            \n",
        "            # Train discriminator\n",
        "            real_images= real_images.to(device)\n",
        "            loss_d, real_score, fake_score = train_discriminator(real_images)\n",
        "            \n",
        "            # Train generator\n",
        "            loss_g, latent = train_generator()\n",
        "            \n",
        "        # Record losses & scores\n",
        "        losses_g.append(loss_g)\n",
        "        losses_d.append(loss_d)\n",
        "        real_scores.append(real_score)\n",
        "        fake_scores.append(fake_score)\n",
        "        \n",
        "        # Log losses & scores (last batch)\n",
        "        print(\"Epoch [{}/{}], loss_g: {}, loss_d: {}, real_score: {}, fake_score: {}\".format(\n",
        "            epoch+1, epochs, loss_g, loss_d, real_score, fake_score))\n",
        "    \n",
        "        # Save generated images\n",
        "        save_samples(sample_path, epoch+start_idx, latent, show=False)\n",
        "\n",
        "        torch.save({\n",
        "            'epoch': epoch+start_idx,\n",
        "            'generator_state_dict': generator.state_dict(),\n",
        "            'discriminator_state_dict': discriminator.state_dict(),\n",
        "            'optimizer_G_state_dict': optimizer_G.state_dict(),\n",
        "            'optimizer_D_state_dict': optimizer_D.state_dict()\n",
        "            }, os.path.join(model_path,f\"model_{epoch+start_idx}.tar\"))\n",
        "    \n",
        "    return losses_g, losses_d, real_scores, fake_scores"
      ],
      "execution_count": 60,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vH57-BktaJ8H"
      },
      "source": [
        "model = fit(epochs=n_epochs, lr=lr, model_path=model_path,sample_path = sample_path)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZhoEbZnJLBw3"
      },
      "source": [
        "# **Image generation + FID**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7wEg5YwjkKNR"
      },
      "source": [
        "def generate_dataset(output_path,num_of_images, batch_size):\n",
        "    \"\"\"Dataset generation\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        output_path: string\n",
        "            path of the output\n",
        "        num_of_images: int\n",
        "            number of images to generate\n",
        "        batch_size:\n",
        "            generator input batch size of latent tensors\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        (loss, real_score, fake_score): Tuple\n",
        "            discriminator loss, discriminator score on real images,\n",
        "            discriminator score on fake images,\n",
        "    \"\"\"\n",
        "    # check whether the output directory created\n",
        "    os.makedirs(output_path, exist_ok=True)\n",
        "    # number of saved image in its output name\n",
        "    saved_num = 0\n",
        "    # saving loop\n",
        "    for batch_num in range(num_of_images//batch_size):\n",
        "        # form random noise vector\n",
        "        latent_vec = torch.randn(batch_size, latent_size, 1,1).to(device)\n",
        "        # generate images from noise\n",
        "        fake_images = generator(latent_vec).cpu().detach().permute(0,2,3,1)\n",
        "        # image normalization from [-1,1] to [0,1]\n",
        "        fake_images = denorm(fake_images)\n",
        "        # image contrasting\n",
        "        fake_images = contrasting(fake_images)\n",
        "        # image conversion\n",
        "        fake_images = img_as_ubyte(fake_images)\n",
        "        # saving\n",
        "        for image_num in range(batch_size):\n",
        "            saved_num += 1\n",
        "            imsave(os.path.join(output_path,f\"{saved_num}.jpg\"),fake_images[image_num])\n"
      ],
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HnneNQXX1BCv"
      },
      "source": [
        "# load checkpoint dict\n",
        "checkpoint_path = \"drive/MyDrive/model_100.tar\"\n",
        "checkpoint = torch.load(checkpoint_path, map_location=device)\n",
        "\n",
        "# initialize model with trained parameters\n",
        "generator.load_state_dict(checkpoint['generator_state_dict'])\n",
        "generator.to(device)\n",
        "generator.eval()\n",
        "\n",
        "# generation\n",
        "generate_dataset(\"generated_100\",10000,10)"
      ],
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rua5A4JiNSZU"
      },
      "source": [
        "!pip install pytorch_fid"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k2MxQ1fe6B1N"
      },
      "source": [
        "!python -m pytorch_fid generated_100 dataset/cats"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}